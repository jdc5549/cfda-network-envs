import time
import networkx as nx
import numpy as np
from gym import spaces
from netcasc_gym_env import NetworkCascEnv
import sys
import os
import copy
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

sys.path.append('./marl/')
import marl
from marl.agent import MABAgent
from marl.exploration.expls import UCB1, EpsGreedy
from marl.policy.policies import RandomPolicy,TargetedPolicy,RTMixedPolicy,SubactMinimaxQCriticPolicy
from marl.tools import gymSpace2dim,ncr,get_combinatorial_actions
from marl.agent.agent import Agent
# sys.path.append('../stable-baselines3')
# from stable_baselines3.common.vec_env import SubprocVecEnv,VecMonitor
# from stable_baselines3.common.utils import set_random_seed

from scm import SCM

import torch
# import torch.nn as nn
# import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from collections import defaultdict,deque
from utils import create_random_nets,get_rtmixed_nash
from models import MLP_Critic

def process_batch(args):
    state,dex_expltr,aex_expltr,training_env,all_actions = args

    ego_actions = [pi(state) for pi in agent_policies]
    ego_atk_idx = all_actions.index(ego_actions[0])
    ego_def_idx = all_actions.index(ego_actions[1])

    batch_dex_act_idx = def_expltr.exploration(def_expltr.policy)
    dex_actions = [all_actions[batch_dex_act_idx], ego_actions[1]]

    batch_aex_act_idx = atk_expltr.exploration(atk_expltr.policy)
    aex_actions = [ego_actions[0], all_actions[batch_aex_act_idx]]

    _ = training_env.reset()
    _, (ego_reward, _), _, _ = training_env.step(ego_actions)
    _ = training_env.reset()
    _, (dex_reward, _), _, _ = training_env.step(dex_actions)
    _ = training_env.reset()
    _, (aex_reward, _), _, _ = training_env.step(aex_actions)

    return (ego_reward, dex_reward, aex_reward,batch_dex_act_idx,batch_aex_act_idx)

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Netcasc Gym Args')
    parser.add_argument("--training_steps",default=1000,type=int,help='Number of steps to train the model if we are training.')
    parser.add_argument("--batch_size",default=100,type=int,help='Number of samples to gather before updating the model. Exploitability calced wrt to average over batch.')
    parser.add_argument("--cascade_type",default='threshold',type=str,help='Type of cascading dynamics.')
    parser.add_argument("--learning_rate",default=0.1,type=float,help='Reinforcement Learning rate.')
    parser.add_argument("--sched_step",default=0.5,type=int,help='How often to reduce the learning rate for training NN model')
    parser.add_argument("--sched_gamma",default=0.1,type=float,help='How much to reduce the learning rate after shed_step steps')
    parser.add_argument("--exploration_type",default='UCB',type=str,help="Which exploration strategy to use")
    parser.add_argument("--eps_deb",default=1.0,type=float,help='How much exploration to process when exploration begins')
    parser.add_argument("--eps_fin",default=0.1,type=float,help='How much exploration to process when exploration ends')
    parser.add_argument("--deb_expl",default=0,type=float,help='When (as fraction of total training steps) to begin exploration')
    parser.add_argument("--fin_expl",default=0.5,type=float,help='When (as fraction of total training steps) to end exploration')
    parser.add_argument("--exploited_type",default='NN',type=str,help='What type of agent to train exploiters against. Valid choices are: NN,Random,Heuristic, and RTMixed')
    parser.add_argument("--ego_model_path",default=None,type=str,help='dir where nn model to load is for the ego agents')
    parser.add_argument("--net_path",default=None,type=str,help='dir to load the network topology from')
    parser.add_argument("--ego_exp_name",default=None,type=str,help='name of ego model')
    parser.add_argument("--net_size",default=10,type=int,help='Number of nodes in the power network.')
    parser.add_argument("--exp_name",default='my_exp',type=str,help='Name of experiment that will be associated with log and model files.')
    parser.add_argument("--device",default='cpu',type=str,help='Device to perform training on.')

    args = parser.parse_args()
    num_acts = ncr(args.net_size,2)

    p = 2/args.net_size
    training_env = NetworkCascEnv(args.net_size,p,p,'File',6,cascade_type=args.cascade_type,filename=args.net_path)
    obs_sp = training_env.observation_space
    act_sp = training_env.action_space
    if args.exploration_type == 'EpsGreedy':
        exploration_atk = EpsGreedy(eps_deb=args.eps_deb, eps_fin=args.eps_fin, deb_expl=0, fin_expl=args.fin_expl,device=args.device)
        exploration_def = EpsGreedy(eps_deb=args.eps_deb, eps_fin=args.eps_fin, deb_expl=0, fin_expl=args.fin_expl,device=args.device)
    elif args.exploration_type == 'UCB':
        exploration_atk = UCB1(num_acts,device=args.device)
        exploration_def = UCB1(num_acts,device=args.device)        
    else:
        print(f'Exploration Process {args.exploration_type} not recognized.')
        exit()


    all_actions= get_combinatorial_actions(args.net_size,2)
    state = [training_env.scm.G.degree(n) for n in training_env.scm.G.nodes]
    if args.exploited_type == 'NN':
        state_dict = torch.load(os.path.join(args.ego_model_path,'final.pt'))
        # for key,val in state_dict.items():
        #     print(f'{key}: {val.shape}')
        # exit()
        embed_size = state_dict['act_embedding.weight'].shape[1]
        hidden_size = state_dict['mlp_layers.0.weight'].shape[0]
        output_size = state_dict['output_layer.weight'].shape[0]
        depth = int(len([key for key in state_dict if 'mlp_layers' in key])/2)

        q_model = MLP_Critic(embed_size,hidden_size,output_size,num_mlp_layers=depth)
        q_model.load_state_dict(state_dict)
        q_model.to(args.device)

        atk_policy = SubactMinimaxQCriticPolicy(q_model,action_space=act_sp,player=0,all_actions=all_actions,eval_mode=True,device=args.device,model_path=args.ego_model_path)
        def_policy = SubactMinimaxQCriticPolicy(q_model,action_space=act_sp,player=1,all_actions=all_actions,eval_mode=True,device=args.device,model_path=args.ego_model_path)
        agent_policies = [atk_policy,def_policy]
    elif args.exploited_type == 'Random':
        random_policy = RandomPolicy(act_sp,num_actions=num_samples,all_actions= all_actions)
        agent_policies = [copy.deepcopy(random_policy),copy.deepcopy(random_policy)]
    elif args.exploited_type == 'Targeted':
        heuristic_policy = TargetedPolicy(act_sp,num_actions=num_samples,all_actions= all_actions)
        agent_policies = [copy.deepcopy(heuristic_policy),copy.deepcopy(heuristic_policy)]
    elif args.exploited_type == 'RTMixed':
        rtmixed_fn = f'data/Ego/{args.net_size}C2/rtmixed_eq.npy'
        if os.path.exists(rtmixed_fn):
            print(f'Loading Saved RTMixed Params from: {rtmixed_fn}')
            pt_atk,pt_def = np.load(rtmixed_fn)
        else:
            random_policy = RandomPolicy(act_sp,all_actions= all_actions)
            targeted_policy = TargetedPolicy(act_sp,all_actions=all_actions)
            obs = training_env.reset()
            pt_atk,pt_def = get_rtmixed_nash(training_env,targeted_policy,random_policy)
            np.save(rtmixed_fn,(pt_atk,pt_def))
        agent_policies = [RTMixedPolicy(act_sp,pt_atk,state,all_actions=all_actions),
        RTMixedPolicy(act_sp,pt_def,state,all_actions=all_actions)]
        nash_eq_dir = f'data/Ego/{args.net_size}C2/ego_NashEQ/{args.cascade_type}casc_NashEQs/'
        if os.path.exists(nash_eq_dir):
            random_policy = RandomPolicy(act_sp,all_actions= all_actions)
            targeted_policy = TargetedPolicy(act_sp,all_actions=all_actions)
            nash_eq_fn = nash_eq_dir + 'eq.npy'
            eqs = np.load(nash_eq_fn)
            obs = training_env.reset()
            tact = targeted_policy([obs])
            targeted_pi = np.zeros(gymSpace2dim(act_sp))
            targeted_pi[targeted_policy.sorted_idx] = 1
            random_pi = 1/25*np.ones(gymSpace2dim(act_sp))
            mixed_pi = np.array([pt_atk*targeted_pi + (1-pt_atk)*random_pi,pt_def*targeted_pi + (1-pt_def)*random_pi])
            from scipy.stats import entropy
            kl = entropy(eqs.flatten(),mixed_pi.flatten())
            print(f'KL Divergence for RTMixed: {kl}')
    # mas = MARL_with_exploiters(agent_list,[def_expltr,atk_expltr],log_dir='marl_logs',name=args.exp_name,obs=[ob[0] for ob in env.obs],nash_policies=eqs,
    #         exploited=args.exploited_type,explt_opp_update_freq=args.test_freq)
    def_expltr = MABAgent(act_sp,args.net_size,exploration=exploration_def,lr=args.learning_rate,sched_step=args.sched_step*args.training_steps,gamma=args.sched_gamma,device=args.device)
    atk_expltr = MABAgent(act_sp,args.net_size,exploration=exploration_atk,lr=args.learning_rate,sched_step=args.sched_step*args.training_steps,gamma=args.sched_gamma,device=args.device)
    log_dir = f'logs/exploiters/'
    if not os.path.isdir(log_dir):
        os.mkdir(log_dir)
    hparams = {"training_steps": args.training_steps, "learning_rate": args.learning_rate,"ego_model": args.ego_exp_name,"net_size": args.net_size}
    #"sched_step": args.sched_step, "sched_gamma":args.sched_gamma,        
    writer = SummaryWriter(os.path.join(log_dir, args.exp_name))
    writer.add_hparams(hparams,{'dummy/dummy': 0},run_name=None)#'hparams')



    model_dir = f'models/exploiters/{args.ego_exp_name}/{args.exp_name}/'
    if not os.path.isdir(model_dir):
        os.makedirs(model_dir,exist_ok=True)

    best_exploitability = -1
    def_expltr.exploration.reset(args.training_steps)
    atk_expltr.exploration.reset(args.training_steps)
    #tic = time.perf_counter()
    #num_processes = cpu_count()-2
    #chunk_size, remainder = divmod(args.batch_size,num_processes)
    #args_list = [(agent_policies,state,def_expltr,atk_expltr,copy.deepcopy(training_env),all_actions) for i in range(args.batch_size)]
    toc = time.perf_counter()
    #print(f'Initialized Args in {toc-tic} seconds.')
    progress_bar = tqdm(total=args.training_steps, desc='Training Progress')
    for t in range(args.training_steps):
        batch_ego_actions = []
        batch_dex_actions = []
        batch_aex_actions = []
        batch_dex_act_idx = torch.zeros((args.batch_size),dtype=torch.long).to(args.device)
        batch_aex_act_idx = torch.zeros((args.batch_size),dtype=torch.long).to(args.device)
        batch_ego_rew = torch.zeros(args.batch_size).to(args.device)
        batch_dex_rew = torch.zeros(args.batch_size).to(args.device)
        batch_aex_rew = torch.zeros(args.batch_size).to(args.device)
        #with tqdm(total=args.batch_size) as pbar:
        for i in range(args.batch_size):
            #get actions from ego and exploiter agents
            ego_actions = [pi(state) for pi in agent_policies]
            ego_atk_idx = all_actions.index(ego_actions[0])
            ego_def_idx = all_actions.index(ego_actions[1])

            batch_ego_actions.append(ego_actions)
            batch_dex_act_idx[i] = def_expltr.exploration(def_expltr.policy)
            dex_actions = [all_actions[batch_dex_act_idx[i]],ego_actions[1]]
            batch_dex_actions.append(dex_actions)
            batch_aex_act_idx[i] = atk_expltr.exploration(atk_expltr.policy)
            aex_actions = [ego_actions[0],all_actions[batch_aex_act_idx[i]]]
            batch_aex_actions.append(aex_actions)
            _ = training_env.reset()
            _,(batch_ego_rew,_),_,_ = training_env.step(ego_actions)
            _ = training_env.reset()
            _, (batch_dex_rew[i],_), _, _ = training_env.step(dex_actions)
            _ = training_env.reset()
            _, (batch_aex_rew[i],_), _, _ = training_env.step(aex_actions)
            #pbar.update()
        #exit()
        # tic = time.perf_counter()
        # with Pool(cpu_count()-2) as pool:
        #     batch_results = pool.map(process_batch, args_list)
        # toc = time.perf_counter()
        # print(f'Time to complete pool: {toc-tic}')

        # for i, (ego_reward, dex_reward, aex_reward,dex_act_idx,aex_act_idx) in enumerate(batch_results):
        #     batch_ego_rew[i] = ego_reward
        #     batch_dex_rew[i] = dex_reward
        #     batch_aex_rew[i] = aex_reward
        #     batch_ego_rew[i] = ego_reward
        #     batch_dex_rew[i] = dex_reward
        #     batch_aex_rew[i] = aex_reward


        #Update exploiters
        dex_reward = torch.mean(batch_dex_rew).item()
        dex_loss = torch.mean(torch.abs(batch_dex_rew - def_expltr.policy.Q(batch_dex_act_idx))).item()
        def_expltr.update_q(batch_dex_rew,batch_dex_act_idx)
        aex_loss = torch.mean(torch.abs(batch_aex_rew - atk_expltr.policy.Q(batch_aex_act_idx))).item()
        atk_expltr.update_q(batch_aex_rew,batch_aex_act_idx)

        exploitability = torch.mean((batch_dex_rew-batch_ego_rew) + (batch_ego_rew-batch_aex_rew)).item()
        writer.add_scalar("Hyperparameters/learning_rate",def_expltr.lr,t)
        writer.add_scalar("Training/dex_loss",dex_loss,t)
        writer.add_scalar("Training/aex_loss",aex_loss,t)
        writer.add_scalar("Training/dex_reward",dex_reward,t)
        writer.add_scalar("Training/exploitability",exploitability,t)
        if exploitability > best_exploitability:
            best_exploitability = exploitability
            fn_atk = os.path.join(model_dir,'best_aex_model.npy')
            np.save(fn_atk,atk_expltr.policy.Q.q_table.cpu().numpy())
            fn_def = os.path.join(model_dir,'best_dex_model.npy')
            np.save(fn_def,def_expltr.policy.Q.q_table.cpu().numpy())
        
        atk_expltr.exploration.update(t)
        def_expltr.exploration.update(t)

        progress_bar.set_postfix({'dex_loss': dex_loss,'aex_loss': aex_loss,'exploitability': exploitability})
        progress_bar.update()

    progress_bar.close()

    fn_atk = os.path.join(model_dir,'final_aex_model.npy')
    np.save(fn_atk,atk_expltr.policy.Q.q_table.cpu().numpy())

    fn_def = os.path.join(model_dir,'final_dex_model.npy')
    np.save(fn_def,def_expltr.policy.Q.q_table.cpu().numpy())

    print(f'Saved trained Exploiter Models to {model_dir}')
    print(f'Best Exploitability Acheived was {best_exploitability}')